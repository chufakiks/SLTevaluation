model:
  target: spamo.t5_slt.FlanT5SLT

  params:
    lr: 6.0e-4
    monitor: val/bleu4
    model_name: google/flan-t5-xl
    cache_dir: null  # Use default HuggingFace cache directory
    inter_hidden: 768
    input_size: 2048
    beam_size: 5
    max_frame_len: 512
    max_txt_len: 64
    frame_sample_rate: 1
    prompt: "Translate the given sentence into {}."
    tuning_type: lora
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    fusion_mode: joint
    warm_up_steps: 0
    cross_modal_align: true
    combined_loss: true
    alpha: 1.0
    use_in_context: true
    num_in_context: 3
