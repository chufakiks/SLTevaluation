#!/bin/sh
### General options
### -- specify queue --
#BSUB -q gpua100
### -- set the job Name --
#BSUB -J ASL_STEM_Wiki_Inference
### -- ask for number of cores (default: 1) --
#BSUB -n 4
### -- specify that the cores must be on the same host --
#BSUB -R "span[hosts=1]"
### -- specify that we need 8GB of memory per core/slot --
#BSUB -R "rusage[mem=8GB]"
### -- request 1 GPU --
#BSUB -gpu "num=1:mode=exclusive_process"
### -- specify that we want the job to get killed if it exceeds 10 GB per core/slot --
#BSUB -M 10GB
### -- set walltime limit: hh:mm --
#BSUB -W 24:00
### -- set the email address --
# please uncomment the following line and put in your e-mail address,
# if you want to receive e-mail notifications on a non-default address
##BSUB -u your_email_address
### -- send notification at start --
#BSUB -B
### -- send notification at completion --
#BSUB -N
### -- Specify the output and error file. %J is the job-id --
### -- -o and -e mean append, -oo and -eo mean overwrite --
#BSUB -o Output_%J.out
#BSUB -e Output_%J.err

# Load necessary modules
module load python3/3.10.12
module load cuda/12.1

# Set up virtual environment
VENV_DIR=/work3/s235253/spamo_env

# Always create fresh environment
echo "Creating virtual environment..."
python3 -m venv $VENV_DIR
source $VENV_DIR/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

cd SLTevaluation

# Run batch inference on ASL STEM Wiki dataset (10% sample)
python batchinferencespamo.py \
    --video_dir /work3/s235253/ASL_STEM_Wiki/videos \
    --checkpoint spamocheckpoint.ckpt \
    --config configs/finetune.yaml \
    --output_dir ./inference_output_asl_stem_wiki \
    --target_lang English \
    --device cuda:0 \
    --spatial_batch_size 64 \
    --motion_batch_size 16 \
    --sample_percent 10 \
    --log_file ./inference_progress.log \
    --save_features